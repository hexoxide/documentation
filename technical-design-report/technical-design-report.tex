% \documentclass[article, onecolumn]{IEEEtran}
% \IEEEoverridecommandlockouts
\documentclass[]{article}

\usepackage{cite}
\usepackage{color, colortbl}
\usepackage{float}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{textcomp}

\usepackage[colorlinks]{hyperref}
\hypersetup{allcolors=blue}

\lstset{breaklines=true}

\newcommand{\tabitem}{~~\llap{-}~~}
\newcommand{\tabitemNum}{~~}

\definecolor{red}{rgb}{1,0.9,0.9}
\definecolor{green}{rgb}{0.9,1,0.9}

\title{Technical Design Report \\
	\large Software for Science - CERN Load Balancing}

\author{\textbf{Team 6 Hexoxide} \\
	Eloy Maduro Clément \\
	Geoffrey van Driessel \\
	Corné Lukken \\
	Bram Tukker}

\date{\textbf{\today} \\
	v1.1}

\begin{document}
\maketitle
\addcontentsline{toc}{section}{\protect\numberline{}Title page} % Include table of contents without partition numbering
\newpage

\section*{Changelog}
\addcontentsline{toc}{section}{\protect\numberline{}Changelog}
\begin{table}[H]
	\begin{center}	
		\begin{tabularx}{\textwidth}{ | l | l | X | }
			\hline
			\textbf{Version} & \textbf{Date} & \textbf{Description} \\ \hline
			
			\multirow{1}{*}{0.2} & 07-11-2018 & \tabitemNum 1. Added information on changing to Debian image. \\
			& & \tabitemNum 2. Added information about investigating Metricbeat for information logging. \\
			& & \tabitemNum 3. Added table of contents. \\
			& & \tabitemNum 4. Added information on how legacy software was setup and used. \\
			& & \tabitemNum 5. Added previously logged metrics and desired metrics. \\ \hline
			
			\multirow{1}{*}{0.3} & 01-12-2018 & \tabitemNum 1. Remove outdated or incoherent information. \\
			& & \tabitemNum 2. Add networking benchmarks. \\ \hline
			
			\multirow{1}{*}{0.4} & 07-12-2018 & \tabitemNum 1. Add installation instructions to report. \\
			& & \tabitemNum 2. Explain 94Mbit synthetic throughput. \\
			& & \tabitemNum 3. Introduction to alternative technologies. \\
			& & \tabitemNum 4. Incorporate research design into TDR. \\
			& & \tabitemNum 5. Add experiment network-baseline details. \\ \hline
			
			\multirow{1}{*}{0.5} & 12-12-2018 & \tabitemNum 1. Add operation diagram for network baseline experiment. \\
			& & \tabitemNum 2. Finish documentation of network baseline experiment. \\ \hline
			
			\multirow{1}{*}{0.6} & 10-01-2019 & \tabitemNum 1. Add round robin ICN experiment. \\ \hline
			
			\multirow{1}{*}{0.7} & 15-01-2019 & \tabitemNum 1. Add source code documentation section. \\
			& & \tabitemNum 2. Add results for round robin ICN. \\ \hline
			
			\multirow{1}{*}{1.0} & 16-01-2019 & \tabitemNum 1. Determined mandatory changes. \\
			& & \tabitemNum 2. Improved hardware \& software setup introduction. \\
			& & \tabitemNum 3. Added networking diagrams. \\
			& & \tabitemNum 4. Described software and system components. \\
			& & \tabitemNum 5. Generalized SSH description. \\
			& & \tabitemNum 6. Described Metricbeat and it’s metrics. \\
			& & \tabitemNum 7. Explained table underlining. \\
			& & \tabitemNum 8. Describe the run operation for the round robin ICN experiment. \\
			& & \tabitemNum 9. Discussion \& results. \\
			& & \tabitemNum 10. MELK-installation-guide reference added \\
			& & \tabitemNum 11. MELK Sub Question. \\
			& & \tabitemNum 12. Added ZooKeeper section. \\ \hline
			\multirow{1}{*}{1.0} & 19-01-2019 & \tabitemNum 13. Added Bibtex for IEEE referencing. \\
			& & \tabitemNum 14. Added CERN official Bibtex document output to reference TDR. \\ \hline
			\multirow{1}{*}{1.0} & 21-01-2019 & \tabitemNum 15. Additional bookmark references for uncounted sections. \\ \hline
		\end{tabularx}
	\end{center}
\end{table}
\newpage

\tableofcontents{}
\newpage

\section*{Introduction}
\addcontentsline{toc}{section}{\protect\numberline{}Introduction}
This document reflects the discovered \& analyzed technical components for the CERN Load Balancing project. Components are identified in previously available documents as well as discoveries of the project team itself. 

Experiments and their setup are detailed so they can be repeated easily in the future, as well as instructions on how to install all required components. This work will introduce new software that can be used in future projects to continue experimenting and make new discoveries. This new software has evolved from previous work which will be briefly touched upon. It is important to note that there is a distinction between the two new systems we provide: the ICN and the Zookeeper implementation.

The rationale for the project will be explained in the introduction along with its common terminology, an overview of past and future events as well as information about previous work.

\subsection*{Alice O$^2$}
\addcontentsline{toc}{subsection}{\protect\numberline{}Alice O$^2$}
CERN is the organization which has build worlds largest particle accelerator know as the Large Hadron Collider (LHC). The LHC, along with its various detectors, is currently receiving a scheduled upgrade during a phase called Long Shutdown 2 (LS2). One of these various detectors is known as A Large Ion Collider Experiment (Alice). This detector will also be receiving an upgrade during LS2 and as a result, the amount of data created by the detector will increase. However, the network behind the detector is currently not capable of processing the data at the increased rate. The Alice O$^2$ project describes the necessary changes to software, hardware and network in order to be able to process the increased data rate of the detector. In preparation of the upgrade, the Alice O$^2$ project was documented in a Technical Design Report (TDR) released by CERN in 2015\cite{Buncic:2011297}.

The CERN Load Balancing project focuses on the network load balancing described in the TDR for Alice O$^2$. The two primary types of computer nodes that will perform the network load balancing operations for the received detector data are known as the First Level Processors (FLPs), and the Event Processing Nodes (EPNs), however, throughout this document these will also be abbreviated in singular form being FLP and EPN. The FLPs will be directly interfacing with the Alice detector on a special device known as an Field Programmable Gate Array (FPGA) and based on triggers from these FPGAs, they will start transmitting data to the EPNs. In turn, the EPNs will receive this data and from the FLPs, however, to correctly process the data all FLP's need to transmit to a single EPN for each event. The load balancing should ensure the correct transmission of so called Sub-Time Frames(STFs), transmitted from the FLPs to the EPNs so they can be processed into Time Frames(TF). The overall Alice O$^2$ system and operation is described in greater detail but this is best read in the TDR\cite{Buncic:2011297} as the currently described information should suffice for further reading of this document.

\subsection*{Previous work}
\addcontentsline{toc}{subsection}{\protect\numberline{}Previous work}

The first work was done in 2017 and the result showed the effectiveness of a simple round-robin algorithm\cite{vander_2016}. The software implementation created to run the required experiments introduced a new third type of computing node named the Information Node. The Information Node was responsible for maintaining a list of available EPNs in the network using the ZooKeeper distributed key-value store\cite{apache_zookeeper}.

Later on, the ZooKeeper configuration was optimized by determining the optimal ticktime while evaluating two new algorithms: Blacklist and Re-Initialization\cite{vander_2017}. Upon evaluating the two algorithms, it became clear that the Blacklist algorithm was superior when compared to the Re-Initialization algorithm.

All work was based on evaluations and experiments that were run on X86 based processors but the work of Puls introduced experiments that could be run on ARM based processors, more notably Raspberry Pi clusters\cite{mitchel_2018}. These clusters could now be used as a more available and lower cost platform to run experiments on.

\begin{center}
	\begin{figure}[H]
		\includegraphics[width=\textwidth]{images/TimelineProject.png}
		\caption{Overview of past and future events with regards to the CERN Load Balancing project.}
		\label{fig:timeline}
	\end{figure}
\end{center}

\subsection*{Original O2Balancer}
\addcontentsline{toc}{subsection}{\protect\numberline{}Original O2Balancer}
\textit{\textbf{Disclaimer: The original O2Balancer had never reached an operational stage between the duration of the Software for Science Load Balancing project 2018/2019 and although it provides details which will help others come a long way, it will take further steps to be able to use the original O2Balancer to run experiments.}}

At the start of this project, several experiments had previously been done on this subject. These previous experiments have led to the creation of existing software. It is important to identify how these software packages worked and reference them, so that they can be used in the future.

To make sure the exact version of the software can be retrieved, the git version commit hashes are listed. To checkout a specific commit hash from a repository, execute “git checkout COMMIT-HASH”.

\begin{table}[H]
	\begin{center}	
		\begin{tabularx}{\textwidth}{ | l | X | l | }
			\hline
			\textbf{Software} & \textbf{Purpose} & \textbf{Commit} \\ \hline
			
			O2Balancer\cite{O2Balancer_github} & Binaries used in experiments containing specific algorithms to be tested. & \href{https://github.com/valvy/O2-Balancer/tree/36eec890a1ec7c236d2d2363c6938dcb59fb4b3e}{36eec89} \\ \hline
			
			BalancerScripts\cite{balancerscripts_github} & Setting up nodes with Ansible and orchestrating an experiment. Gathering and purging log files and creating plots and graphs from log data. & \href{https://github.com/valvy/BalancerScripts/tree/99f7177bdaa4c98db65f17070dfcab350faccdb5}{99f7177} \\ \hline
		\end{tabularx}
		\caption{Original O2Balancer Software.}
		\label{tab:originalbalancer}
	\end{center}
\end{table}

Newer software will likely still be distributed through git, but will be made available through the software-for-science Github organization. The repositories from previous experiments can be found on the organization project\cite{software_for_science_github}.

\subsubsection*{Installation}
\addcontentsline{toc}{subsubsection}{\protect\numberline{}Installation}
Documentation for the O2Balancer was based upon CentOS, as a result it might be difficult to get the software running for other types of Linux distributions although it has been successfully done before. The documentation on how to install \& run the O2Balancer for the Raspbian Linux distribution has been lost but the installation instructions for CentOS are still available\cite{O2BalancerCompilation}. Attempts to retrieve the lost documentation by searching through Github, attempting to contact the author and even rescuing files of previously used SDCards were unsuccessful.

For other distributions with different package managers it should be fairly easy to replace the packages specified for the YUM package manager, however, installing all requirements for the O2Balancer can require a significant amount of system memory (between 8-16 gigabytes).

\subsection*{Verify software installation}
\addcontentsline{toc}{subsubsection}{\protect\numberline{}Verify software installation}
Once the O2Balancer and its dependencies are installed, the installation can be verified by executing execute.sh in the build directory of O2Balancer, however, first ZooKeeper needs to be started.

\begin{lstlisting}[language=bash]
sudo /usr/share/zookeeper/bin/zkServer.sh start
./execute.sh
\end{lstlisting}

\subsection*{Configuration arguments \& yaml files}
\addcontentsline{toc}{subsubsection}{\protect\numberline{}Configuration arguments \& yaml files}
Each individual binary has command line arguments and a configuration file, some of the parameters configured through these two methods might override one or another.

\begin{table}[H]
	\textbf{Information Node}	
	\begin{center}	
		\begin{tabularx}{\textwidth}{ | l | l | X | }
			\hline
			\textbf{Parameter} & \textbf{Data type} & \textbf{Description} \\ \hline
					
			--info-config & string & Path to yaml configuration file. \\ \hline
			--sample-size & int & Size of individual packets send between FLP \& EPN in MB. \\ \hline
			--ip & string & Ip address of ZooKeeper service \\ \hline
		\end{tabularx}
		\caption{Information Node parameters.}
		\label{tab:informationnode}
	\end{center}
\end{table}

\begin{table}[H]
	\textbf{FLP}	
	\begin{center}	
		\begin{tabularx}{\textwidth}{ | l | l | X | }
			\hline
			\textbf{Parameter} & \textbf{Data type} & \textbf{Description} \\ \hline
					
			--flp-config & string & Path to yaml configuration file. \\ \hline
			--restartFairRoot & boolean & Runs reinitialization algorithm if true \\ \hline
			--ip & string & Ip address of ZooKeeper service \\ \hline
		\end{tabularx}
		\caption{FLP parameters.}
		\label{tab:flporiginal}
	\end{center}
\end{table}

\begin{table}[H]
	\textbf{EPN}	
	\begin{center}	
		\begin{tabularx}{\textwidth}{ | l | l | X | }
			\hline
			\textbf{Parameter} & \textbf{Data type} & \textbf{Description} \\ \hline
					
			--epn-config & string & Path to yaml configuration file. \\ \hline
			--amount-flps & int & Amount of FLPs in this experiment the EPN will expect data from. \\ \hline
			--flp-port & int & The network port the EPN will listen on for data from the FLP. \\ \hline
			--ip & string & Ip address of ZooKeeper service \\ \hline
		\end{tabularx}
		\caption{EPN parameters.}
		\label{tab:epnoriginal}
	\end{center}
\end{table}

The execute.sh script will open a total of six windows which should appear, each running a different type of node software. In total there should be three EPNs, twp FLPs \& one Information Node.

\section{Hardware \& software setup}
The Raspberry Pi clusters used in experiments were initially empty. So the Raspberry Pi clusters are configured with an image. To speed up the initial process, one image was made and reused for the other clusters. The required dependencies were also installed on the initial image. A Bash script was developed which helps with the installation of the dependencies. The remaining processes were automated with Ansible. All the scripts to speed up initial pi cluster setup are stored in repository that are publicly accessible and listed within this document. Bash was chosen as an automation language because of its high availability on Linux distributions, Subsequently using Bash instead of Ansible allows to install dependencies on nodes which might not be accessible with Ansible such as isolated virtual machines.

Raspberry Pi nodes are ARM based and this can pose limits on the available software, for instance Logstash will not compile or run on ARM based systems. It is important to take these limits into account and before starting to implement properly investigate software and its availability.

\subsection{Hardware Specifications}
According to the Alice technical design report \cite{Buncic:2011297}, there will be two types of computing nodes in the Alice O$^2$ project after LS2. In total there will be about 2000 nodes in O$^2$, 250 so called FLP nodes and 1750 EPN nodes.

A set of 80 computing devices grouped in units of four has been provided by the Amsterdam University of Applied Sciences (AUAS) for this project. These 80 nodes will be used to simulate network scenarios as close as possible to the actual O$^2$ computing network. The computing devices at AUAS will be Raspberry Pis, specifically, model 3 B+.

In Table \ref{tab:specs} a comparison of differences in hardware is detailed. It is clear that the computing nodes provided by AUAS are vastly inferior to the computing hardware that will become available for Alice O$^2$. These hardware differences must be taken greatly into account during the project and its various experiments.

\begin{table}[H]
	\begin{center}
		\begin{tabular}{ | l | l | l | l | }
			\hline
			\textbf{Component} & \textbf{FLP (250)} & \textbf{EPN (1750)} & \textbf{Raspberry Pi 3 B+ (80)} \\ \hline
			
			CPU & ? x86 & 32 Core x86 & 4 Core ARMv8 \\ \hline
			RAM & 32GB & 128GB & 1GB \\ \hline
			Ethernet & 4 x 10Gbit & 10Gbit & 300Mbit \\ \hline
		\end{tabular}
		\caption{Hardware specifications of computing nodes.}
		\label{tab:specs}
	\end{center}
\end{table}

\subsection{Network}
The network during the experiments consisted of eight Raspberry Pi nodes each with two network interfaces. One of these interfaces had an internet connection through a bridged router, but was segregated from the actual internal network using Network Address Translation (NAT). The initial system design has every component installed on one Raspberry Pi but unfortunately some system components such as Logstash turned out to be incompatible with ARM based processors. A virtual machine running on an X86 based host was configured to resolve any ARM related compatibility issues, however, this host resided outside the NAT segregated network and additional steps using port forwarding and additional NAT were required to resolve them.

The following networking diagram shows an overview of the entire network using Cisco diagramming standards. In this diagram, the host at 192.168.3.2 is the virtual machine responsible for running X86 system components, while the host at 192.168.3.4 is the router responsible for segregating the Raspberry Pis from the rest of the internal network. Each of the firewalls represent the NAT that occurs on two places in the network which will require port forwarding and tunneling later.

\begin{center}
	\begin{figure}[H]
		\includegraphics[width=\textwidth]{images/network-primary}
		\caption{Networking diagram with auxiliary networking interfaces omitted. A diagram with the auxiliary interface is available online\cite{Auxiliary_network}}
		\label{fig:ssh}
	\end{figure}
\end{center}

\subsection{New software introduction}
An environment as similar as possible to the one used at CERN is desired. It has, however, proven unfeasible to use CentOS, due to the limitations the Raspberry pi version has in comparison to the CentOS x86\_64 image. One of the main reasons is the inability to switch between gcc versions. Normally, a tool called SCL provides the switching for specific versions of many development tools. Furthermore, the version of gcc supplied with CentOS is incompatible with the version of boost that is specified in many of the previous experiments. To continue using CentOS, gcc would have to be built from source.

To resolve the issues with the CentOS image, an alternative had to be elected and based on previous experience, Manjaro 17.1 was chosen. Manjaro provides a full featured ARM image for with a large amount of available packages. Manjaro being an Arch based distribution is extremely permissive in adaptive configurations allowing it to closely reflect any other distribution.

Unfortunately, due to the rolling release model of Manjaro, this also led to a lot of issues including, but not limited to: broken packages, unavailable mirrors and missing kernel modules. As an alternative, Raspbian stretch was chosen because of its extensive support and large community, which should allow to more easily mitigate problems. 

Based on the reports of the previous experiments, a table could be compiled which compares the version of libraries from the legacy software to the versions used in the new software. It should be noted that the information about the original O2Balancer was determined to be partially incorrect as it does not use FairMQ itself but instead relies on FairRoot.

\begin{table}[H]
	\begin{center}
		\begin{tabular}{ | l | l | l | }
			\hline			
			\textbf{Library/Tool/OS} & \textbf{Version used} & \textbf{Version Original O2Balancer} \\ \hline
			
			FairMQ & 1.3.6 & 1.1.5 \\ \hline
			ZeroMQ & 4.2.5 & 4.2.1 \\ \hline
			ZooKeeper & 3.4.9 & 3.4.9 \\ \hline
			Cmake & 3.12.4 & 3.11.0 \\ \hline
			Boost & 1.68.0 & 1.66.0 \\ \hline
			Yaml-cpp & NONE & 0.5.2 \\ \hline
			FairLogger & 1.3.0 & NONE \\ \hline
			Compiler & gcc 6.3.0 & gcc 6.3.0 \\	\hline
			OS & Raspbian stretch & Raspbian stretch \\ \hline
		\end{tabular}
		\caption{The installed software dependencies on the Raspberry Pis.}
		\label{tab:librabies}
	\end{center}
\end{table}

\subsection{New software \& system components}
The system consist of different nodes all of which are running Debian stretch as operating system and which are accessible using SSH. The Raspberry Pi based nodes have all dependencies installed using the raspberry-dependency repository and its install scripts. The X86 based virtual machine has different software components installed which will be described later for these components an installation guide is available in the documentation repository.

Among the set of nodes is one new node known as the Information Control Node (ICN). This node will replace the Information Node used in the original O2Balancer. The new ICN node will have additional functionality compared to the original Information Node which is the reason behind the new name. The other nodes in the new software such as the FLP and EPN will have the same functionality as in the original O2Balancer.

\begin{table}[H]
	\begin{center}
		\begin{tabularx}{\textwidth}{ | l | X | }
			\hline			
			\textbf{Repository} & \textbf{Purpose} \\ \hline
			
			raspberry-dependency\cite{hex_dependency} & Detailed instructions listing required Debian packages and automated install script to install further dependencies not installed through package manager. \\ \hline
			BalancerScripts2\cite{hex_scripts} & Small scripts that can be executed from the command line to orchestrate and/or configure software components that allow the experiment to run across different nodes. \\ \hline
			O2-Balancer2\cite{hex_balancer} & Core binaries which run the experiments by configuring and sending data on different channels across the network. \\ \hline
			Documentation\cite{hex_documentation} & Essential documentation on the overall project as well as how to get started with the continuation of our project. \\ \hline
		\end{tabularx}
		\caption{Overview of essential repositories.}
		\label{tab:librabies}
	\end{center}
\end{table}

\subsection{Installation}
To ensure the reproducibility of experiments executed by the new software, a Bash script\cite{hex_dependency} has been developed which will install all the dependencies automatically. Using Bash instead of Ansible ensures the availability of the install script on isolated platforms. The installation instructions are based on Debian distributions and additional effort could be required to be able to use them on different distributions. Along this installation scripts additional documentation is available in the documentation repository which will allow to configure the X86 based system components.

\subsection{SSH}
Remote access to each of the individual nodes is done via SSH in the setup the first node is used a portal to access the other nodes in the network. Using a single node as portal limits the amount of ports that need to be forwarded to the outside network. Additionally, an SSH key from the portal node can be added to every node as known host so that SSH logins no longer require a username \& password after logging into the portal.

The SSH configuration also uses SSH tunneling. This is done to allow access to a web based dashboard running on the X86 based virtual machine.

\begin{center}
	\begin{figure}[H]
		\includegraphics[width=\textwidth]{images/ssh}
		\caption{Example of login in using SSH on remote accessible raspberry pi.}
		\label{fig:ssh}
	\end{figure}
\end{center}

\subsection{Ansible}
To run and configure tests on the pi clusters, some things will need to be configured. Ansible will be used to handle the configuration of each pi, defining which experiments to run and with which parameters to do so. Ansible allows to apply different parameters to different categories of hosts.

\subsection{MELK}
Metricbeat can be used to gather statistics of the nodes. This data can be sent to Logstash, which should have run on the ICN. Then, Logstash can output the data in two ways:
\begin{itemize}
	\itemsep 0em
	\item Plain text file
	\item Elasticsearch
\end{itemize}

To easily visualize the data, Kibana can be used to retrieve the data from Elasticsearch. This service should also run on the ICN. By port forwarding the service on an external machine, Kibana can be viewed inside a web browser.

Due to incompatibilities with Logstash on ARM, both Logstash and ElasticSearch were installed on the X86 virtual machine. The overall experience with MELK was very poor.

Kibana is part of the ELK stack, which is used to properly interpret metrics from the hardware and network during experiments. ELK is short for Elasticsearch, Logstash and Kibana. These three pieces of software process incoming data from hardware components running Metricbeat.

\subsubsection{Elasticsearch}
Elasticsearch is a distributed, RESTful search and analytics engine capable of solving a growing number of use cases. As the core of the Elastic Stack, it centrally stores the data \cite{melk_2019}.

\subsubsection{Logstash}
Logstash is an open source, server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it, and then sends it to Elasticsearch.

\subsubsection{Kibana}
Kibana is able to visualize Elasticsearch data and navigate the Elastic Stack, and can do so using a wide range of visualizations.

\subsubsection{Metricbeat}
Collect metrics from systems and services. From CPU to memory, Redis to NGINX, and much more, Metricbeat is a lightweight way to send system and service statistics. However, in the setup for running O2Balancer experiments, the data gathered is limited to metrics such as CPU load (in percentages), memory usage (in MegaBytes), and network throughput (in KiloBytes per second).

\subsubsection{Data}
While Metricbeat offers a wide range of data to send, in the setup for running O2Balancer experiments, the data gathered is limited to metrics such as:
\begin{itemize}
	\itemsep 0em
	\item CPU load (in percentages)
	\item Memory usage (in MegaBytes)
	\item Network throughput (in KiloBytes per second)
	\item Network latency (in milliseconds)
\end{itemize}

\subsubsection{Visualizations}
Kibana offers several types of visualizations: charts such as area, heat map, horizontal and vertical bar and pie. For the purpose of the experiments and their effects on the hardware and network, line charts proved to be most effective in visualising the relevant data, because these showed changes over time.
These graphs are then displayed in a grid formation on a dashboard, which created an ordered overview of all measured statistics.

\begin{center}
	\begin{figure}[H]
		\includegraphics[width=\textwidth]{images/melk}
		\caption{Overview of MELK stack.}
		\label{fig:ssh}
	\end{figure}
\end{center}

\subsubsection{Logging metrics}
The decision to use Metricbeat was heavily based on it’s extensive selection of different metrics. The following overview of metrics shows what was intended to be used to evaluate different experiments.

\begin{table}[H]
	\begin{center}
		\begin{tabular}{ | c | c | c | }
			\hline
			\textbf{\#} & \textbf{Metric} & \textbf{Measure} \\ \hline
			
			1 & Temperatures & degrees Celsius ($^\circ$C) \\ \hline
			2 & Throughput & KiloBytes per second (KB/s) \\ \hline
			3 & Scaling governer / Core clock & MegaHertz (MHz) \\ \hline
			4 & Memory usage & MegaBytes (MB) \\ \hline
			5 & CPU usage & Percentage (\%) \\ \hline
			6 & Swap & MegaBytes (MB) \\ \hline
			7 & Fail-over & Number of Nodes (NoN)\\ \hline
		\end{tabular}
		\caption{Metric that will be used in the experiments.}
		\label{tab:metrics}
	\end{center}
\end{table}


\subsection{Source code documentation}
Beside the regular documentation, the source code itself can also be documented. This helps to understand what the code does. This is especially important for the developers, since they might have to modify the logic or add additional features.

This project uses Doxygen to create the documentation. Doxygen is a library that can automatically generate documentation by scanning through the source file for specific comments.Furthermore, Doxygen can extract the file structure of the project. The output will be in the form of an HTML webpage or multiple LaTeX files. 

\subsubsection{Generate}
In order to use Doxygen for this project, it should be installed on the machine beforehand. It’s important to note that Doxygen will be executed through CMake. So the path to the library should be findable for CMake.

Normally, the documentation will be automatically generated when building the project. However, this can be disabled through the following command:
\begin{lstlisting}[language=bash]
cmake -DENABLE_DOXY=OFF
\end{lstlisting}

\subsubsection{Access}
After Doxygen has been executed, the documentation can be found in the ‘docs’ directory, located in the root directory of the project.

To access the HTML version, open ‘index.html’ inside a web browser. The file is located in the ‘html’ directory. Here follows a overview of the most important sections:
\begin{itemize}
	\itemsep 0em
	\item \textbf{Main Page}: Contains the README.md file of the project.
	\item \textbf{Classes \textrangle{} Class List}: Shows a list of all the classes.
	\item \textbf{Classes \textrangle{} Class Members}: Shows a list of all the functions and variables of the classes.
	\item \textbf{Files \textrangle{} File List}: Overview of the project structure.
\end{itemize}

\subsubsection{Modify}
There are several ways to add comments in the source code which will be detected by doxygen. Here follows the template comments used in this project.
\begin{itemize}
\item \textbf{Block}
\begin{lstlisting}[language=C++]
/*
* [description]
* [more description]
*/
[entity]
\end{lstlisting}

\item \textbf{Brief description (for classes)}
\begin{lstlisting}[language=C++]
/// [description]
[entity]
\end{lstlisting}

\item \textbf{One line}
\begin{lstlisting}[language=C++]
/** [description] */
[entity]
\end{lstlisting}

\item \textbf{After member}
\begin{lstlisting}
[member] /**< [description] */
\end{lstlisting}

\end{itemize}

The full documentation of Doxygen is available online \cite{doxygen}

\section{Synthetic benchmarks}
To measure the maximum throughput that can be achieved on the network, the \hyperref[sec:ref04]{iPerf3 [4]} tool was used to measure the throughput for 10 seconds. The measurement was repeated 50 times. The results show that the network throughput has a relatively low variance and is almost always consistently 94Mbit/s.

The 94Mbit of throughput was expected. Even though the Raspberry Pi has a 1Gbit interface, this is limited to 300Mbit due to sharing a USB 2.0 host controller. This limited throughput is due to the Raspberry Pi’s being connected via a 100Mbit switch.

\begin{center}
	\begin{figure}[H]
		\includegraphics[width=\textwidth]{images/network-benchmark}
		\caption{Distribution of synthetic benchmark throughput.}
		\label{fig:ssh}
	\end{figure}
\end{center}

\section{Research design}
Before diving into the research, this paragraph describes the general problem of the research. The problem is a specific problem of CERN, but the research itself should be applicable to other similar problems.

\begin{center}
	\begin{figure}[H]
		\includegraphics[width=\textwidth]{images/research-model}
		\caption{A Schematic representation of the research (focussed on the actual experiments).}
		\label{fig:ssh}
	\end{figure}
\end{center}

\subsection{Main Research Question} 
The focus of the project is to deliver a scalable, performent and autonoom load balancing system. And also to conduct experiments to gather information about the reliability of the system.
At the initial state of the project we defined the main research question as following: \textit{“How do different load-balancing algorithms compare in transferring a continuous data stream across parallel many-to-one streams?”}

\subsection{Sub Research Question}
In order to answer the main research question, a few subquestion are established to divide the whole project into smaller parts. These are the subquestions:
\begin{itemize}
	\itemsep 0em
	\item Which algorithms have been developed already and what are the results?
	\item What are potential alternatives on which experiments can be run?
	\item Which metrics are available to the validate the quality of the experiments?
	\item How can the metrics be visualised using Kibana?
\end{itemize}


\section{Experiments}
Experiments are documented in 4 parts starting with a general introduction/description. After that, the configuration is detailed and the necessary information is provided to revalidate the experiment. An operational overview and description is given and finally the results are measured. The experiments will only detail preliminary results and a conclusive total overview of results. How they evaluate together can be found in the results chapter. 

The necessary information to revalidate an experiment can be found by using the references in the configuration section.

In the tables describing the experiments, the specific results which have the same throughput as the maximum determined in the synthetic benchmark will be underlined. The following short table is an example of that underlining.

\begin{table}[H]
	\textbf{Example for underlining synthetic benchmark equivalent throughput}
	\begin{center}
		\begin{tabular}{ | l | l | l | }
			\hline
			\textbf{Rate (Hertz)} & \textbf{Message size (Bytes)} & \textbf{Percentage loss (\%)} \\ \hline
			
			200 & 58000 & 0.00 \\ \hline
			\underline{200} & \underline{58750} & \underline{0.49} \\ \hline
		\end{tabular}
		\caption{Underlining of the synthetic benchmark equivalent allows for quickly comparing results.}
		\label{tab:benchmark}
	\end{center}
\end{table}

\subsection{Network throughput baseline}
This experiment tests network throughput using the FairMQ application layers after having measured the maximum throughput that could be achieved on the specific hardware using the synthetic benchmark. The absolute maximum throughput that can be achieved is 94Mbit as demonstrated with iperf3.

The 94 Mbit throughput can be translated into a 11.75 Mbyte throughput. With the message rate of 200Hz, it can be determined that the network should be able to sustain a message size of 58750 bytes. The number of FLPs and the message size was varied during this experiment.

The experiment should run for 30 seconds and is validated with tight timing constraints. The total experiment time should be no less than 29.7 seconds and no more than 30.3. Failure of a run to be within these timing constraints means it will not be used as part of the results.

\begin{center}
	\begin{figure}[H]
		\includegraphics[width=\textwidth]{images/Results-gathering-flow-diagram.png}
		\caption{Flow diagram of run step during experiments}
		\label{fig:FlowDiagram}
	\end{figure}
\end{center}

\subsubsection{Configuration}

\begin{table}[H]
	\begin{center}
		\begin{tabular}{ | l | l | l | l |}
			\hline
			Rate & 200Hz & \multicolumn{2}{ c | }{\textbf{Node}} \\ \hline
			
			Messages & 6000 & \textbf{Type} & \textbf{Ip} \\ \hline
			Runtime & 30 seconds & ICN & 192.168.1.4 \\ \hline
			Message size & Variable & FLP 1 & 192.168.1.3 \\ \hline
			Source commit & \hyperref[sec:ref10]{cbaebb9 [10]} & EPN & 192.168.1.2 \\ \hline
			Documentation &  \hyperref[sec:ref11]{cdbb643 [11]} & FLP 2 & 192.168.1.5 \\ \hline
			Arguments & \hyperref[sec:appendix01]{appendix} & FLP 3 & 192.168.1.6 \\ \hline
		\end{tabular}
		\caption{The specific configuration lists essential parameters as well as the specific commit referencing to the source code.}
		\label{tab:configbaseline}
	\end{center}
\end{table}

The experiment consisted of nine different configurations were both the message size and the number of FLPs in the network were varied. Each configuration was run for 30 iterations, so additional statistical analysis could be performed if required.

\begin{table}[H]
	\textbf{Configuration overview}
	\begin{center}
		\begin{tabular}{ | l | l | l | l | }
			\hline
			\textbf{\#} & \textbf{Message size (Bytes)} & \textbf{Rate (Hertz)} & \textbf{Number of FLPs} \\ \hline
			
			1 & 58000 & 200 & 1 \\ \hline
			\underline{2} & \underline{58750} & \underline{200} & \underline{1} \\ \hline
			3 & 59500 & 200 & 1 \\ \hline
			4 & 29000 & 200 & 2 \\ \hline
			\underline{5} & \underline{29375} & \underline{200} & \underline{2} \\ \hline
			6 & 29750 & 200 & 2 \\ \hline
			7 & 19333 & 200 & 3 \\ \hline
			\underline{8} & \underline{19583} & \underline{200} & \underline{3} \\ \hline
			9 & 19833 & 200 & 3 \\ \hline
		\end{tabular}
		\caption{All nine configurations during the network baseline experiment.}
		\label{tab:configoverviewround}
	\end{center}
\end{table}

\subsubsection{Operation}
The operation of the experiment involved a two step procedure, where in the first step the binaries are configured and, in the second step the actual experiment is run. During this experiment, the ICN’s available channels are hard coded into the binary and could only be changed at compile time. 

The three different core Binaries are started at the same time and the EPN \& FLP will wait for incoming messages while the ICN starts counting down a ten second wait period. After the grace period, the ICN will send the EPN channel configuration to all FLPs. After the message has been sent, the ICN will wait an additional two seconds for the FLPs to configure the newly received channels. When the ICN has waited for the FLPs to be configured, the experiment will run.

\begin{center}
	\begin{figure}[H]
		\includegraphics[width=\textwidth]{images/no-zookeeper-flow-n-base-exp}
		\caption{PreRun configuration stage depicts different operations across each type of node in time.}
		\label{fig:ssh}
	\end{figure}
\end{center}

The network is shown for each of the possible configurations during the experiment, which indicates the physical location of each node in the network. All connections between devices are 100Mbit.

\begin{table}[H]
	\textbf{Network setup}
	\begin{center}
		\begin{tabular}{ | c | c | c | }
			\hline
			\textbf{1 FLP} & \textbf{2 FLP} & \textbf{3 FLP} \\ \hline
			
			\includegraphics[width=0.3\textwidth]{images/network-baseline-1} & \includegraphics[width=0.3\textwidth]{images/network-baseline-2} & \includegraphics[width=0.3\textwidth]{images/network-baseline-3} \\ \hline
		\end{tabular}
		\caption{Network diagrams for each number of FLP configurations.}
		\label{tab:networksetupbaseline}
	\end{center}
\end{table}

\subsubsection{Results}
During the synthetic benchmarks, the maximum network bandwidth was determined to be 94Mbit. In accordance to this synthetic throughput, a maximum message size of 58750 bytes and one FLP is expected to have close to zero or zero loss. In a similar fashion, a message size of 29375 bytes with two FLPs was expected to have similar loss. 

During the experiment it became clear that having multiple FLPs further limits the amount of available bandwidth available for throughput between FLPs and EPNs. Attempting to sustain a throughput of 93 Mbits with more than one FLP results in significant loss of messages. Furthermore, increasing the message size beyond the equivalent of 93 Mbit throughput also results in significant loss of messages. 93 Mbit is close but not equivalent of 94 Mbit. This lower throughput is likely due to the network traffic from ICN to FLP \& from EPN to ICN because these network connections aren’t present in the synthetic benchmark.

The results shows that the number of FLPs has an impact on the maximum throughput that can be achieved and should be taken into account in further experiments. 

\begin{table}[H]
	\textbf{Mean message loss per configuration - 6000 messages at an rate of 200Hz (30 seconds)}
	\begin{center}
		\begin{tabularx}{\textwidth}{ | X | X | X | X | }
			\hline
			\textbf{Number of FLPs} & \textbf{Message size (per FLP)} & \textbf{Mean loss (total)} & \textbf{Mean loss (\%)} \\ \hline
			
			1 & 58000 & 2.333333333 & 0.03 \\ \hline
			\underline{1} & \underline{58750} & \underline{42.13333333} & \underline{0.70} \\ \hline
			1 & 59500 & 119.7333333 & 1.99 \\ \hline
			
			2 & 29000 & 148.3666667 & 2.47 \\ \hline
			\underline{2} & \underline{2937}5 & \underline{121.4333333} & \underline{2.02} \\ \hline
			2 & 29750 & 120.8666667 & 2.01 \\ \hline
			
			3 & 19333 & 196.1 & 3.26 \\ \hline
			\underline{3} & \underline{19583} & \underline{198.6333333} & \underline{3.31} \\ \hline
			3 & 19833 & 885.4333333 & 14.75 \\ \hline
		\end{tabularx}
		\caption{Message loss during the network baseline with 200Hz event rate.}
		\label{tab:resultsbaseline}
	\end{center}
\end{table}

\subsection{Round Robin ICN}
This experiment tests the achievable throughput with minimal packet loss using the same algorithm as performed with previous version of the software. Each of the available EPNs is selected in turn by incrementing an iterator and returning it to the start if the last element has been accessed. 

Eight nodes are configured during this experiment and the node ratio of 1:6 with regards to the FLP and EPNs is achieved. Both 50Hz and 200Hz event rates will be tested to completely cover the event operating range of the network after LS2.

Each configuration is run for ten iterations to achieve a normal distribution should advanced statistical analysis be required later. Additionally, each configuration gets one iteration where the total operation time is set to three minutes instead of 30 seconds so that any variation caused by iteration time can be detected. This will also help determining if an iteration time of 30 seconds is sufficient as this has been used as iteration time in past experiments as well as these experiments. The rationale is that the short iterations could hide potential network bottlenecks that are mitigated using network buffers. However, network buffers can only mitigate such a network bottleneck for a short duration so, as a result the longer iteration time should discover these problems.


\begin{lstlisting}[language=C++]
/**
 * Determines the next appropriate channel the FLP should use 
 * to send data to the EPN.
 * @return The desired channel.
 */
uint64_t InformationControlNode::determineChannel()
{
	uint64_t index = 0;

	// currentChannel initialization 
	if(currentChannel == nullptr)
	{
		currentChannel = channels.at(0);
	}

	if(currentChannel == channels.back())
	{
		currentChannel = channels.front();
	}
	else 
	{
		auto it = std::find(channels.begin(), channels.end(), currentChannel);
		std::advance (it, 1);
		index = std::distance(channels.begin(), it);
		currentChannel = channels.at(index);
	}

	return currentChannel->index;
}
\end{lstlisting}

\subsubsection{Configuration}

\begin{table}[H]
	\begin{center}
		\begin{tabular}{ | l | l | l | l |}
			\hline
			Rate & 50Hz / 200Hz & \multicolumn{2}{ c | }{\textbf{Node}} \\ \hline
			
			Messages & 1500 / 6000 & \textbf{Type} & \textbf{Ip} \\ \hline
			Runtime & 30 seconds & ICN & 192.168.1.1 \\ \hline
			Message size & Variable & FLP 1 & 192.168.1.2 \\ \hline
			Source commit & \hyperref[sec:ref10]{5914f78 [10]} & EPN 1 / FLP 2 & 192.168.1.3 \\ \hline
			Documentation &  \hyperref[sec:ref11]{347173e [11]} & EPN 2 & 192.168.1.4 \\ \hline
			Arguments & \hyperref[sec:appendix02]{appendix} & EPN 3 & 192.168.1.5 \\ \hline
			\multicolumn{2}{ | l | }{\multirow{3}{*}{}} & EPN 4 & 192.168.1.6 \\
			\multicolumn{2}{ | l | }{} & EPN 5 & 192.168.1.7 \\
			\multicolumn{2}{ | l | }{} & EPN 6 & 192.168.1.8 \\ \hline
		\end{tabular}
		\caption{The specific configuration lists essential parameters as well as the specific commit referencing to the source code.}
		\label{tab:configroundrobin}
	\end{center}
\end{table}

A total of fourteen different configurations were tested during this experiment. The message size, event rate and the amount of FLPs in the network were varied. The differences between these configurations should demonstrate the effects on the effective bandwidth of the network with an event rate of 200Hz.

\begin{table}[H]
	\textbf{Configuration overview}	
	\begin{center}
		\begin{tabular}{ | l | l | l | l | }
			\hline
			\textbf{\#} & \textbf{Message size (Bytes)} & \textbf{Rate (Hertz)} & \textbf{Number of FLPs} \\ \hline
			
			1 & 232000 & 50 & 1 \\ \hline
			\underline{2} & \underline{235000} & \underline{50} & \underline{1} \\ \hline
			3 & 238000 & 50 & 1 \\ \hline
			4 & 58000 & 200 & 1 \\ \hline
			\underline{5} & \underline{58750} & \underline{200} & \underline{1} \\ \hline
			6 & 59500 & 200 & 1 \\ \hline
			7 & 65000 & 200 & 1 \\ \hline
			8 & 116000 & 50 & 2 \\ \hline
			\underline{9} & \underline{117500} & \underline{50} & \underline{2} \\ \hline
			10 & 119000 & 50 & 2 \\ \hline
			11 & 29000 & 200 & 2 \\ \hline
			\underline{12} & \underline{29375} & \underline{200} & \underline{2} \\ \hline
			13 & 29750 & 200 & 2 \\ \hline
			14 & 32500 & 200 & 2 \\ \hline
		\end{tabular}
		\caption{All 14 configurations during the round robin ICN experiment.}
		\label{tab:configoverviewround}
	\end{center}
\end{table}

In the results section the configuration numbers will be reused as a method to uniquely identify configurations and as a result it should be easier to cross-reference and get a good overview of how configurations compare.

\subsubsection{Operation}
The operation is similar to the network baseline experiment but additional automatic configuration has been added to simplify the process. The overall operation still consist of a two step procedure, but the EPNs have additional features that allow them to report their listening channel to the ICN. This allows to ICN to maintain a list of EPNs that are available which can then be iterated over in a round robin fashion.

During the PreRun step, the EPN will wait ten seconds after being started and will then proceed by sending its channel data to the ICN. This initial delay allows the ICN to be started without very tight timing constraints as delays between nodes could differ in a network. The ICN is configured to be dormant until it receives the first channel. After receiving the first channel it will allow for additional channels to be added during a ten second period. After this period it will forward all the channels to the FLPs in the network and it will wait two seconds before continuing. During this two second period, the FLPs will have time to configure their channels. With the FLP channels configured, the PreRun step is finished and the run step is entered.

\begin{center}
	\begin{figure}[H]
		\includegraphics[width=\textwidth]{images/no-zookeeper-flow}
		\caption{PreRun configuration stage depicts different operations across each type of node in time.}
		\label{fig:ssh}
	\end{figure}
\end{center}

During the run step, the ICN will monitor the rate of transmission. Whenever the FLP should send the next message to the currently selected EPN, the ICN will send a message to the FLP. This message will contain a heartbeat ID as well as the selected EPN that should receive the message. The ICN will then increment the list of EPNs as a means to select the next one for the following message and the FLPs will send the configured amount of bytes to the EPN. When the EPN has successfully received the messages from the FLPs, it will send an acknowledgement to the ICN. Should an EPN receive two messages with different heartbeat ID’s, it will discard the messages and trigger an out-of-order error. As a result, it will not send an acknowledgement to the ICN for that heartbeat.

This run step procedure allows differentiation between messages that were lost due to high bandwidth \& network buffers or due to timing errors. The expectation is that the increased event rate of 200Hz will increase the amount of out-of-order/timing errors with an equivalent total bandwidth.

To more correctly measure the effectiveness of the round robin algorithm at 50 and 200Hz, all network communication between ICN to FLP and EPN to ICN will be transmitted on the auxiliary networking interface.

\begin{table}[H]
	\textbf{Network setup}
	\begin{center}
		\begin{tabular}{ | c | c | }
			\hline
			\textbf{Primary interface} & \textbf{Auxiliary interface} \\ \hline
			
			\includegraphics[width=0.45\textwidth]{images/network-primary-interface} & \includegraphics[width=0.45\textwidth]{images/network-auxiliary-interface} \\ \hline
		\end{tabular}
		\caption{Network interfaces that are available to nodes.}
		\label{tab:networkround}
	\end{center}
\end{table}

\subsubsection{Results}
Losses across different configurations where there is one FLP in the network are extremely similar to the network throughput baseline experiment. This is likely due to the inability to cause out-of-order errors as there is only a single FLP in the network.

When introducing the second FLP into the network losses start to occur at even lower throughput than with the synthetic benchmark.

\begin{table}[H]
	\textbf{Mean message loss - 1500 messages sent at an rate of 50hz (30 seconds)}	
	\begin{center}
		\begin{tabularx}{\textwidth}{ | X | X | X | X | X | }
			\hline
			\textbf{\#} & \textbf{Number of FLPs} & \textbf{Message size (per FLP)} & \textbf{Mean loss (total)} & \textbf{Mean loss (\%)} \\ \hline
			
			1 & 1 & 232000 & 0 & 0.00 \\ \hline
			\underline{2} & \underline{1} & \underline{235000} & \underline{0} & \underline{0.00} \\ \hline
			3 & 1 & 238000 & 0 & 0.00 \\ \hline
			8 & 2 & 116000 & 7.44 & 0.50 \\ \hline
			\underline{9} & \underline{2} & \underline{117500} & \underline{5.11} & \underline{0.34}\\ \hline
			10 & 2 & 119000 & 11.11 & 0.74 \\ \hline
		\end{tabularx}
		\caption{Comparing loss of configurations with an 50Hz event rate.}
		\label{tab:resultsroundone}
	\end{center}
\end{table}

The losses with two FLPs are higher with an event rate of 200Hz. All of the losses were due to out-of-order errors, directly showing that the tighter timing constraints increase the amount of errors that occur at the same bandwidth.

\begin{table}[H]
	\textbf{Mean message loss - 6000 messages sent at an rate of 200hz (30 seconds)}	
	\begin{center}
		\begin{tabularx}{\textwidth}{ | X | X | X | X | X | }
			\hline
			\textbf{\#} & \textbf{Number of FLPs} & \textbf{Message size (per FLP)} & \textbf{Mean loss (total)} & \textbf{Mean loss (\%)} \\ \hline
			
			4 & 1 & 58000 & 0 & 0.00 \\ \hline
			\underline{5} & \underline{1} & \underline{58750} & \underline{0} & \underline{0.00} \\ \hline
			6 & 1 & 59650 & 0 & 0.00 \\ \hline
			7 & 1 & 65000 & 441.89 & 7.35 \\ \hline
			11 & 2 & 29000 & 30.56 & 0.51 \\ \hline
			\underline{12} & \underline{2} & \underline{29375} & \underline{52.67} & \underline{0.87}\\ \hline
			13 & 2 & 29750 & 53.34 & 0.87 \\ \hline
			14 & 2 & 32500 & 85.56 & 1.42 \\ \hline
		\end{tabularx}
		\caption{Comparing loss of configurations with an 200Hz event rate.}
		\label{tab:resultsroundtwo}
	\end{center}
\end{table}

\subsection{Comparing long \& short runs}
Similar configurations should result in similar loss, even if the duration is varied, on the condition that the duration of the test is sufficient to correctly measure the desired characteristics. The results from comparing short \& long runs, however, show that there is a measurable difference. The results highlighted in red have a higher long run percentage loss than the short run equivalent, while the results highlighted in green have a lower long run percentage loss over the short run.

\begin{table}[H]
	\textbf{Comparing losses of 30 second (short) \& 3 minute (long) runsw}	
	\begin{center}
		\begin{tabular}{ | l | l | l | l | }
			\hline
			\textbf{\#} & \textbf{Mean loss short-run (\%)} & \textbf{Mean loss long-run (\%)} \\ \hline
			
			1 & 0.00 & 0.00 \\ \hline
			2 & 0.00 & 0.00 \\ \hline
			\rowcolor{red}
			3 & 0.00 & 0.27 \\ \hline
			4 & 0.00 & 0.00 \\ \hline
			5 & 0.00 & 0.00 \\ \hline
			\rowcolor{red}
			6 & 0.00 & 1.23 \\ \hline
			\rowcolor{red}
			7 & 7.35 & 9.63 \\ \hline
			\rowcolor{green}
			8 & 0.50 & 0.00 \\ \hline
			\rowcolor{green}
			9 & 0.34 & 0.00 \\ \hline
			\rowcolor{green}
			10 & 0.74 & 0.01 \\ \hline
			\rowcolor{green}
			11 & 0.51 & 0.06 \\ \hline
			\rowcolor{green}
			12 & 0.88 & 0.01 \\ \hline
			\rowcolor{green}
			13 & 0.89 & 0.06 \\ \hline
			\rowcolor{green}
			14 & 1.39 & 0.21 \\ \hline
		\end{tabular}
		\caption{Effects of different iteration times on the percentage of loss during experiments.}
		\label{tab:resultsroundabsolute}
	\end{center}
\end{table}

In all the green highlighted configurations, there were two FLPs in the network. Moreover, all the losses were due to so-called out-of-order errors. An out-of-order error occurs when an EPN receives two messages from the FLPs but each one with a different heartbeat. As a result, the EPN invalidates the data and discards it so it can continue receiving new messages. Out-of-order errors are more likely to occur with tighter timing constraints or with fewer EPNs in the network. The lower percentage loss on long runs might indicate that most of the out-of-order events occur during the beginning of the experiment and eventually settle out or at least become less frequent.

In all the red highlighted configurations, the total size of accumulated messages multiplied by the rate of events exceeded the determined maximum throughput as determined in the synthetic benchmark. The results show that in the short run, exceeding these network limits does not result in detected losses. However, in the long runs this does result in losses. This is likely due to network buffers which are temporarily capable of resolving the exceeded limits.


\section{O2Balancer2 Zookeeper version}
The second system we provide is the round-robin whitelist loadbalancer, using \hyperref[sec:ref12]{Zookeeper [12]} for node registration and distribution. This new implementation offers a robust system: the order of which FLP/EPN/ICN starts-up or shuts-down has no impact on the system. With a failover of any of the three nodes the system will continue working: after the node comes back online, it will be part of the system again. 

It is imported to note that the ICN is much different in this system than in the O2Balancer2 ICN version. The ICN in this implementation \textbf{only} sends heartbeats to the FLPs.

The whitelist algorithm has a different approach than the previous \hyperref[sec:ref13]{version [13]}. The previous version overrides a fairmq class to allow for removing a channel from the channel list contained in MQProgOptions.  The new version solves this by maintaining a list of the online nodes so it is known which channels not to use. This allows for significant faster reinitializing of the node, since the channel doesn’t have to be broken down, instead the channels stay unused until the node comes back online in which case the node will be added to the list of online nodes. 

Multiple small improvements have been made, such as: no overriding of fairmq classes(so that is compatible to further versions), FLP only retrieves zk values of new EPNs (instead of all EPNs with each watcher trigger), uses watchers, can detect restarted nodes, relieved network load by using direct channels from FLP to ICN(instead of using zookeeper), and finally a simpler file structure.

The software can be found on the \hyperref[sec:ref12]{zookeepertje [12]} branch from the O2Balancer2 repo. Unfortunately there are no experiments conducted with this zookeeper implementation due to time constraints.

\subsection{ZooKeeper throughput experiment} 
The round-robin whitelist implementation using Zookeeper has the following throughput:
\begin{itemize}
	\itemsep 0em
	\item Using packet size: 232000 bytes
	\item 2 flps - 5 epns - 60 tfps - measured input: 18 packets (3,67MB per epn = 18,8 MB/s total)
	\item 1 flps - 5 epns - 60 tfps - measured input: 12,5 packets (2,16 MB per epn = 10,8 MB/s total)
\end{itemize}

\section{Results}
In this section the results from different experiments are compared as well as data from the synthetic benchmark. The information will be used to draw conclusion and recommend further work.

During the synthetic benchmark the network was able to sustain a throughput of 94Mbit while during the network baseline only 93Mbit could be achieved with an single FLP, however the network baseline only used the primary networking interface for all data and ran at an event rate of 200Hz. The additional results from the round robin ICN experiment show that the 94Mbit throughput can be sustained at both 50 \& 200Hz when using the auxiliary networking interface for communication between ICN to FLP and EPN to ICN.

\begin{center}
	\begin{figure}[H]
		\includegraphics[width=\textwidth]{"images/The effect on loss due to the unavailability of the auxiliary network interface"}
	\end{figure}
\end{center}

This sustained throughput of 94Mbit was only possible with a single FLP in the network while with two FLPs there were still losses at 93Mbit throughput, although these losses were around 0.5\% for both 50 \& 200Hz event rates. The experiments show significant data that the 200Hz event rate could be sustained by an round robin algorithm with only a small reduction in network throughput.

\begin{center}
	\begin{figure}[H]
		\includegraphics[width=\textwidth]{"images/Effects of increased event rates on the loss for Round Robin with 2 FLP's"}
	\end{figure}
\end{center}

Finally the variation of iteration time from 30 seconds to 3 minutes has shown that this can have an effect of the loss that will be measured during the experiment. The total losses measured for both 30 second and 3 minute iterations are relatively low (always below 10\% and regularly below 3\%), however, the difference between the two iteration times can be as much as 650\%. This is an substantial difference that shows that other iteration times than 30 seconds should be considered in upcoming experiments.

\begin{center}
	\begin{figure}[H]
		\includegraphics[width=\textwidth]{"images/Effects of iteration time during round robin ICN experiment for 1 FLP"}
	\end{figure}
\end{center}

\begin{center}
	\begin{figure}[H]
		\includegraphics[width=\textwidth]{"images/Effects of iteration time during round robin ICN experiment for 2 FLP"}
	\end{figure}
\end{center}

\subsection{Metricbeat}
During the round robin ICN experiments, Metricbeat was used on the Raspberry Pi nodes to record metric data. Unfortunately, the results from this data proved unusable due to unreliability as well as the Metricbeat process requiring a lot of system resources. The Metricbeat process occasionally caused 65\% CPU utilization. Additionally, many graphs tended to be unreliable, making it hard to correlate them. Many of the nodes stopped correctly reporting metrics after a period of time which required the restarting of the Metricbeat process to be resolved.

\begin{center}
	\begin{figure}[H]
		\includegraphics[width=\textwidth]{images/metric-load}
		\caption{Graph depicting the occasional 65\% CPU utilization which was determined to be Metricbeat using tools as Htop.}
		\label{fig:ssh}
	\end{figure}
\end{center}

\section{Conclusion}
200Hz event rates should be manageable with only a small reduction in throughput when using the round robin algorithm, additional experiments with more nodes are required.

Metricbeat could be useful but it requires a lot of effort to be correctly configured in combination with Logstash, Elasticsearch \& Kibana. However, it could be unfeasible to run this service on ARM based nodes (such as Raspberry Pis) and additional work is required to determine if it is feasible.

Future experiments should take multiple iteration times into account as this parameter can affect results. Perhaps previous experiments should be repeated with different iteration times.

\section{Discussion}
Additional experiments with more nodes are required to better determine the effects of the round robin algorithm at 200Hz event rate. During this experiments, additional metrics would be of great value, but the overall feasibility of metricbeats and the entire MELK stack needs to be properly evaluated, especially for the ARM platform. In addition to the increased amount of nodes a larger variety of iteration times could also benefit the results.
An important aspect of the zookeeper system that could be tested in a further study is whether all the flps send all the subtime frames of a heartbeat to the same epn, and if they keep doing that with a prolonged run-time (1 hour+). Also a reliability comparison between the two systems used (Zookeeper vs ICN) would be interesting.

\pdfbookmark[1]{References}{References}
\bibliographystyle{IEEEtran}
\bibliography{bibliography}

% \begin{thebibliography}{}
% 	\label{sec:ref01}\bibitem{}\href{https://github.com/hexoxide/O2-Balancer}{O2-Balancer}
% 	\label{sec:ref02}\bibitem{}\href{https://github.com/valvy/BalancerScripts}{Balancer Scripts}
% 	\label{sec:ref03}\bibitem{}\href{http://cds.cern.ch/record/2011297/files/Alice-TDR-019.pdf?version=3}{Alice Technical Design Report}
% 	\label{sec:ref04}\bibitem{}\href{https://iperf.fr/}{iPerf}
% 	\label{sec:ref05}\bibitem{}\href{https://github.com/hexoxide/documentation/blob/master/draw-io/Network-Primary-Auxillary-Dec-14.png}{Networking diagram with auxiliary interface}
% 	\label{sec:ref06}\bibitem{}\href{https://github.com/hexoxide/raspberry-dependency}{Raspberry Dependency}
% 	\label{sec:ref07}\bibitem{}\href{https://github.com/hexoxide/BalancerScripts2}{Balancer Scripts 2}
% 	\label{sec:ref08}\bibitem{}\href{https://github.com/hexoxide/O2-Balancer2}{O2-Balancer 2}
% 	\label{sec:ref09}\bibitem{}\href{https://github.com/hexoxide/documentation}{Documentation}	
% 	\label{sec:ref10}\bibitem{}\href{https://github.com/hexoxide/O2-Balancer2/commits/experiment-1}{O2-Balancer 2 Commits}
% 	\label{sec:ref11}\bibitem{}\href{https://github.com/hexoxide/documentation/tree/cdbb643c6ebc875e73e8c1c2433c6d14cda05aca}{Documentation cdbb643}
% 	\label{sec:ref12}\bibitem{}\href{https://github.com/hexoxide/O2-Balancer2/tree/zookeepertje}{O2-Balancer 2 ZooKeepertje}
% 	\label{sec:ref13}\bibitem{}\href{https://github.com/valvy/}{GitHub Valvy Profile}
% \end{thebibliography}

\section{Appendix}

\subsection{Commands to replicate network throughput baseline}
\label{sec:appendix01}
XXXXXX = Message size \\
YYY = Number of FLPs in the network

\subsubsection{Commands for experiment with single network card}
\begin{lstlisting}[language=bash]
./icn/icn --severity trace --verbosity high --id 1 --iterations 6000 --rate 200 --channel-config name=broadcast,type=pub,method=bind,rateLogging=0,address=tcp://*:5005 name=feedback,type=pull,method=bind,rateLogging=0,address=tcp://*:5000
\end{lstlisting}

\begin{lstlisting}[language=bash]
./flp/flp --severity trace --verbosity high --id 1 --bytes-per-message XXXXXX --channel-config name=broadcast,type=sub,method=connect,rateLogging=1,address=tcp://192.168.1.4:5005
\end{lstlisting}

\begin{lstlisting}[language=bash]
./epn/epn --severity trace --verbosity high --id 1 --num-flp YYY --channel-config name=1,type=pull,method=bind,address=tcp://*:5555,rateLogging=1 name=feedback,type=push,method=connect,address=tcp://192.168.1.4:5000
\end{lstlisting}

\subsubsection{Commands for experiment with dual network cards}
\begin{lstlisting}[language=bash]
./icn/icn --severity trace --verbosity high --id 1 --iterations 6000 --rate 200 --channel-config name=broadcast,type=pub,method=bind,rateLogging=0,address=tcp://*:5005 name=feedback,type=pull,method=bind,rateLogging=0,address=tcp://*:5000
\end{lstlisting}

\begin{lstlisting}[language=bash]
./flp/flp --severity trace --verbosity high --id 1 --bytes-per-message XXXXXX --channel-config name=broadcast,type=sub,method=connect,rateLogging=1,address=tcp://10.42.0.138:5005
\end{lstlisting}

\begin{lstlisting}[language=bash]
./epn/epn --severity trace --verbosity high --id 1 --num-flp YYY --channel-config name=1,type=pull,method=bind,address=tcp://*:5555,rateLogging=1 name=feedback,type=push,method=connect,address=tcp://10.42.0.138:5000
\end{lstlisting}

\subsection{Commands to replicate round robin}
\label{sec:appendix02}
XXXXXX = Message size \\
YYY = Rate of the experiment \\
ZZZ = Number of iterations \\
WWW = EPN number

\subsubsection{Build configuration}
\begin{lstlisting}[language=bash]
cmake -DCMAKE_BUILD_TYPE=Release -DENABLE_DOXY=false ..
make 
make test
\end{lstlisting}

\subsubsection{1 FLP}
\begin{lstlisting}[language=bash]
./icn/icn --control static --severity trace --verbosity high --id 1 --iterations ZZZ --rate YYY --channel-config name=broadcast,type=pub,method=bind,rateLogging=1,address=tcp://*:5005 name=feedback,type=pull,method=bind,rateLogging=1,address=tcp://*:5000
\end{lstlisting}

\begin{lstlisting}[language=bash]
./flp/flp --severity trace --verbosity high --id 1 --bytes-per-message XXXXXX --channel-config name=broadcast,type=sub,method=connect,rateLogging=1,address=tcp://10.42.0.1:5005
\end{lstlisting}

\begin{lstlisting}[language=bash]
./epn/epn --control static --severity trace --verbosity high --id WWW --primary-interface eth0 --num-flp 1 --channel-config name=WWW,type=pull,method=bind,address=tcp://*:5555,rateLogging=1 name=feedback,type=push,method=connect,address=tcp://10.42.0.1:5000
\end{lstlisting}

\subsubsection{2 FLP}
\begin{lstlisting}[language=bash]
./icn/icn --control static --severity trace --verbosity high --id 1 --iterations ZZZ --rate YYY --channel-config name=broadcast,type=pub,method=bind,rateLogging=1,address=tcp://*:5005 name=feedback,type=pull,method=bind,rateLogging=1,address=tcp://*:5000
\end{lstlisting}

\begin{lstlisting}[language=bash]
./flp/flp --severity trace --verbosity high --id 1 --bytes-per-message XXXXXX --channel-config name=broadcast,type=sub,method=connect,rateLogging=1,address=tcp://10.42.0.1:5005
\end{lstlisting}

\begin{lstlisting}[language=bash]
./epn/epn --control static --severity trace --verbosity high --id WWW --primary-interface eth0 --num-flp 2 --channel-config name=WWW,type=pull,method=bind,address=tcp://*:5555,rateLogging=1 name=feedback,type=push,method=connect,address=tcp://10.42.0.1:5000
\end{lstlisting}

\subsubsection{Commands for experiment with round-robin whitelist Zookeeper }
git checkout zookeepertje
\begin{lstlisting}[language=bash]
node 1:
sudo /usr/share/zookeeper/bin/zkServer.sh start
./icn --severity trace --verbosity high --id 1 --iterations 6000 --rate 200 --channel-config name=broadcast,type=pub,method=bind,rateLogging=0,address=tcp://*:5005
\end{lstlisting}

\begin{lstlisting}[language=bash]
node 2 and 5:
./flp --severity trace --verbosity high --id 1 --channel-config name=broadcast,type=sub,method=connect,rateLogging=1,address=tcp://<ip address of icn>:5005
\end{lstlisting}

\begin{lstlisting}[language=bash]
node 3,4,6,7 and 8
./epn --severity trace --verbosity high --id 1 --channel-config name=1,type=pull,method=bind,address=tcp://<own ip address>:5555,rateLogging=1
\end{lstlisting}


\end{document}
